PS C:\Users\220250572> C:\Users\220250572\AppData\Local\Programs\Python\Python310\python.exe -m venv C:\envs\pose_ergo  
PS C:\Users\220250572> C:\envs\pose_ergo\Scripts\Activate.ps1  

(pose_ergo) PS C:\Users\220250572> pip install ultralytics
install all the lib.

Dataset_architecture
Dataset/
├── Images/
│   ├── Train/
│   │   ├── Safe_Back-Support_2.jpg
│   │   ├── ...
│   └── Validation/
│       ├── ...
├── labels/
│   ├── train/
│   │   ├── Safe_Back-Support_2.txt
│   │   ├── ...
│   └── validation/
│       ├── ...
└── person_keypoints_coco17.yaml

# Use CVAT (computer Vision Annotation tool)- Create Skelton with coco17 keypoints and Export to coco17 keypoints 1.1 format.
# Standard COCO-17 order

COCO_KEYPOINTS = [

    "nose", "left_eye", "right_eye", "left_ear", "right_ear",

    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",

    "left_wrist", "right_wrist", "left_hip", "right_hip",

    "left_knee", "right_knee", "left_ankle", "right_ankle"

]

C:\Users\220250572\Desktop\Ergonmics\ergoposekeypoints\annotations
if Coco keypoints were mixed up then need to run the fix_coco17_keypoints.py script. (o/P pl see the file person_keypoints_coco17_fixed.JSON)

# convert person_keypoints_coco17_fixed.json to YOLO.txt i used GPT5 to convert. (pl check the output in the following path
C:\Users\220250572\Desktop\Ergonmics\Dataset\labels\train\'.txt'
output : 
0 0.5430153846153847 0.5054188481675392 0.3596923076923077 0.6636649214659686 0.46876923076923077 0.2080628272251309 2 0.46070769230769226 0.17908376963350783 1 0.4450769230769231 0.1894502617801047 2 0.3937846153846154 0.17358638743455498 1 0.37384615384615383 0.20641361256544502 2 0.3631692307692308 0.26905759162303666 1 0.36467692307692307 0.32175392670157066 2 0.4740615384615384 0.4618324607329843 1 0.3973846153846154 0.4844502617801047 2 0.5695076923076923 0.47717277486911 2 0.5613846153846154 0.52 2 0.44110769230769237 0.482696335078534 1 0.3872923076923077 0.5915706806282722 2 0.7228615384615384 0.5916230366492147 1 0.7156923076923076 0.6431937172774869 2 0.661723076923077 0.7856544502617802 2 0.6868615384615384 0.8372513089005236 2


Breakdown of your YOLOv8 pose `.txt` annotation line for COCO 17 keypoints:

### **YOLOv8 Pose Annotation Format (COCO 17 Keypoints)**

Each line in the `.txt` file is structured as:

class x_center y_center width height x1 y1 v1 x2 y2 v2 ... x17 y17 v17

- **class**: The class index (usually `0` for person).
- **x_center, y_center, width, height**: Normalized bounding box coordinates (values between 0 and 1).
- **x1 y1 v1 ... x17 y17 v17**:  
  - `xN`, `yN`: Normalized coordinates (0-1) for each keypoint (COCO order).
  - `vN`: Visibility flag for each keypoint:
    - `0`: not labeled
    - `1`: labeled but not visible
    - `2`: labeled and visible

### **Your Example Explained**

```
0 0.5430153846153847 0.5054188481675392 0.3596923076923077 0.6636649214659686
0.46876923076923077 0.2080628272251309 2
0.46070769230769226 0.17908376963350783 1
0.4450769230769231 0.1894502617801047 2
...
0.6868615384615384 0.8372513089005236 2
```

- **0**: person class
- **0.543... 0.505... 0.359... 0.663...**: bounding box (center x, center y, width, height)
- **0.468... 0.208... 2**: nose (x, y, visible)
- **0.460... 0.179... 1**: left_eye (x, y, labeled but not visible)
- **0.445... 0.189... 2**: right_eye (x, y, visible)
- ... (continues for all 17 keypoints in COCO order)

---

### **COCO 17 Keypoint Order**
1. nose
2. left_eye
3. right_eye
4. left_ear
5. right_ear
6. left_shoulder
7. right_shoulder
8. left_elbow
9. right_elbow
10. left_wrist
11. right_wrist
12. left_hip
13. right_hip
14. left_knee
15. right_knee
16. left_ankle
17. right_ankle

---

**Summary:**  
Your annotation line encodes the bounding box and all 17 keypoints for one person in the image, with visibility flags.  
This is the correct format for YOLOv8 pose training.

and after validate to check the result were as per coco 17 pl validate_coco17_json.py
oputput looks like:
(pose_ergo) PS C:\Users\220250572\Desktop\Ergonmics\ergoposekeypoints\annotations> python "validate_coco17_json.py"
=== COCO-17 Validation Report ===
Images: 13 | Annotations: 13 | Categories: 1

✅ All checks passed. JSON is COCO-17 compliant.

Convert 

person_keypoints_coco17.yaml

train: C:/Users/220250572/Desktop/Ergonmics/Dataset/images/train     # Folder with training images
val: C:/Users/220250572/Desktop/Ergonmics/Dataset/images/validation         # Folder with validation images

# Number of classes (person)
nc: 1

# Class names
names:
  0: person

# Number of keypoints
kpt_shape: [17, 3]  # 17 keypoints, each with (x, y, v)

# Keypoint names (COCO order)
keypoints:
  - nose
  - left_eye
  - right_eye
  - left_ear
  - right_ear
  - left_shoulder
  - right_shoulder
  - left_elbow
  - right_elbow
  - left_wrist
  - right_wrist
  - left_hip
  - right_hip
  - left_knee
  - right_knee
  - left_ankle
  - right_ankle

(pose_ergo) PS C:\Users\220250572\OneDrive - Regal Rexnord\Desktop\Ergonmics\Dataset> python "python check_yolo_pose_dataset.py"

=== YOLOv8 Pose Dataset Validation Summary ===

Split: train
Total images: 13
Total label files found: 13
Missing label files: 0
Empty label files: 0

Split: val
Total images: 13
Total label files found: 13
Missing label files: 0
Empty label files: 0

(pose_ergo) PS C:\Users\220250572\Desktop\Ergonmics> yolo pose train data="C:/Users/220250572/Desktop/Ergonmics/Dataset/person_keypoints_coco17.yaml" model=yolov8n-pose.pt epochs=20 imgsz=640

  Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size
      20/20         0G      1.026      1.856     0.2309     0.5215      1.664          8        640: 100% ━━━━━━━━━━━━ 1/1 0.1it/s 8.7s
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 0.1it/s 9.2s
                   all          8          8      0.985          1      0.995      0.611      0.985          1      0.995      0.495

20 epochs completed in 0.073 hours.
Optimizer stripped from C:\Users\220250572\Desktop\Ergonmics\runs\pose\train\weights\last.pt, 6.8MB
Optimizer stripped from C:\Users\220250572\Desktop\Ergonmics\runs\pose\train\weights\best.pt, 6.8MB

Validating C:\Users\220250572\Desktop\Ergonmics\runs\pose\train\weights\best.pt...
Ultralytics 8.3.202  Python-3.10.11 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i5-1145G7 2.60GHz)
YOLOv8n-pose summary (fused): 81 layers, 3,289,964 parameters, 0 gradients, 9.2 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 0.2it/s 4.1s
                   all          8          8      0.985          1      0.995      0.611      0.985          1      0.995      0.495
Speed: 3.4ms preprocess, 430.8ms inference, 0.0ms loss, 28.4ms postprocess per image
Results saved to C:\Users\220250572\Desktop\Ergonmics\runs\pose\train
 Learn more at https://docs.ultralytics.com/modes/train

Inference:
Validation Results (Summary)

Images: 8 (used for validation)
Instances: 8 (detected persons)
Box Precision (P): 0.985
Box Recall (R): 1.0
Box mAP50: 0.995
Box mAP50-95: 0.611
Pose Precision (P): 0.985
Pose Recall (R): 1.0
Pose mAP50: 0.995
Pose mAP50-95: 0.495

High Precision & Recall: Your model is detecting and localizing people and their keypoints very well on your validation set.
mAP50-95: This is a stricter metric; your scores are reasonable for a small, custom dataset.

test the model:
 PS C:\Users\220250572\Desktop\Ergonmics> yolo pose predict model="C:/Users/220250572/
Desktop/Ergonmics/runs/pose/train/weights/best.pt" source="C:\Users\220250572\Desktop\Ergonmics\images\Safe_Back_support_1
.jpg" 
Ultralytics 8.3.202  Python-3.10.11 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i5-1145G7 2.60GHz)
YOLOv8n-pose summary (fused): 81 layers, 3,289,964 parameters, 0 gradients, 9.2 GFLOPs

image 1/1 C:\Users\220250572\Desktop\Ergonmics\images\Safe_Back_support_1.jpg: 640x512 1 person, 86.7ms
Speed: 57.3ms preprocess, 86.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 512)
Results saved to C:\Users\220250572\Desktop\Ergonmics\runs\pose\predict


for Testing Image
(pose_ergo) PS C:\Users\220250572\Desktop\Ergonmics> python "test_rebascore_incl_load_AS_image.py"
REBA Worksheet Additions:
Load/Force: 0 = <5kg, 1 = 5-10kg, 2 = >10kg
Enter load/force score: 10   
Activity: 0 = static, 1 = repeated/small, 2 = rapid/unstable
Enter activity score: 1
REBA Score: 27
Inference: Posture shows very high ergonomic risk. Immediate action required.
Annotated image with skeleton saved at: C:/Users/220250572/Desktop/Ergonmics/output/Unsafe_Lifting_1_pose_skeleton.jpg
Excel and PDF reports have been saved in C:/Users/220250572/Desktop/Ergonmics/output/

For testing Video
(pose_ergo) PS C:\Users\220250572\Desktop\Ergonmics> python "test_rebascore_incl_load_AS_video.py"
REBA Worksheet Additions:
Load/Force: 0 = <5kg, 1 = 5-10kg, 2 = >10kg
Enter load/force score: 0
Activity: 0 = static, 1 = repeated/small, 2 = rapid/unstable
Enter activity score: 1
Annotated video, Excel, and PDF reports saved in C:/Users/220250572/Desktop/Ergonmics/output/

Pip list (Lib)
(pose_ergo) PS C:\Users\220250572\Desktop\Ergonmics> pip list
Package            Version
------------------ -----------
certifi            2025.8.3
charset-normalizer 3.4.3
contourpy          1.3.2
cycler             0.12.1
et_xmlfile         2.0.0
filelock           3.19.1
fonttools          4.60.0
fsspec             2025.9.0
idna               3.10
Jinja2             3.1.6
kiwisolver         1.4.9
MarkupSafe         3.0.2
matplotlib         3.10.6
mpmath             1.3.0
networkx           3.4.2
numpy              2.2.6
opencv-python      4.12.0.88
openpyxl           3.1.5
packaging          25.0
pandas             2.3.2
pillow             11.3.0
pip                23.0.1
polars             1.33.1
psutil             7.1.0
PyMuPDF            1.26.4
pyparsing          3.2.5
python-dateutil    2.9.0.post0
pytz               2025.2
PyYAML             6.0.2
requests           2.32.5
scipy              1.15.3
setuptools         65.5.0
six                1.17.0
slim               0.1
sympy              1.14.0
torch              2.8.0
torchvision        0.23.0
typing_extensions  4.15.0
tzdata             2025.2
ultralytics        8.3.202
ultralytics-thop   2.0.17
urllib3            2.5.0

